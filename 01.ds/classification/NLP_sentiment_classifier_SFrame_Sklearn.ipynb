{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with SciKit Learn\n",
    "* SFrame for data preprocessing and Feature Engineering\n",
    "* Scikit-Learn CountVerctorizer for NLP feature extraction\n",
    "* Scikit-Learn LogisticRegression for model fitting\n",
    "\n",
    "* Interpret weights (coefficients)\n",
    "* Make predictions (both class and probability)\n",
    "* Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import string\n",
    "import json\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "#from nltk.corpus import stopwords # python3 only\n",
    "from sklearn import feature_extraction\n",
    "from sklearn import linear_model\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products = pd.read_csv('data/amazon_baby.csv', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Planetwise Flannel Wipes</td>\n",
       "      <td>These flannel wipes are OK, but in my opinion ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Planetwise Wipe Pouch</td>\n",
       "      <td>it came early and was not disappointed. i love...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annas Dream Full Quilt with 2 Shams</td>\n",
       "      <td>Very soft and comfortable and warmer than it l...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>This is a product well worth the purchase.  I ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Stop Pacifier Sucking without tears with Thumb...</td>\n",
       "      <td>All of my kids have cried non-stop when I trie...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                           Planetwise Flannel Wipes   \n",
       "1                              Planetwise Wipe Pouch   \n",
       "2                Annas Dream Full Quilt with 2 Shams   \n",
       "3  Stop Pacifier Sucking without tears with Thumb...   \n",
       "4  Stop Pacifier Sucking without tears with Thumb...   \n",
       "\n",
       "                                              review  rating  \n",
       "0  These flannel wipes are OK, but in my opinion ...       3  \n",
       "1  it came early and was not disappointed. i love...       5  \n",
       "2  Very soft and comfortable and warmer than it l...       5  \n",
       "3  This is a product well worth the purchase.  I ...       5  \n",
       "4  All of my kids have cried non-stop when I trie...       5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sentiment analysis, **slice out** reviews with *rating = 3*, assuming they have neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183531\n",
      "166752\n",
      "\n",
      "Empty reviews:  777\n",
      "165975\n"
     ]
    }
   ],
   "source": [
    "print len(products)\n",
    "products = products[products['rating'] != 3]\n",
    "print len(products)\n",
    "print\n",
    "print 'Empty reviews: ', sum(pd.isnull(products.review))\n",
    "products = products[~pd.isnull(products.review).values]\n",
    "print len(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, some of the classical steps in an NLP pipeline:\n",
    "1. Remove punctuation.\n",
    "2. Remove non-letters\n",
    "3. Remove stop words\n",
    "4. Convert to lower case.\n",
    "5. Transform the reviews into word-counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return str(text).translate(None, string.punctuation) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(raw_documents, stop=None):\n",
    "    \"\"\"\n",
    "    Returns the X feature matrix in scipy sparse array format.\n",
    "    To convert it to a regular numpy array format use .toarray()\n",
    "    \"\"\"\n",
    "    vectorizer = feature_extraction.text.CountVectorizer(analyzer = \"word\",   \\\n",
    "                                                         tokenizer = None,    \\\n",
    "                                                         preprocessor = None, \\\n",
    "                                                         stop_words = stop,   \\\n",
    "                                                         min_df = 0,          \\\n",
    "                                                         max_features = 100000) \n",
    "\n",
    "    # Learn the vocabulary dictionary and return term-document matrix.\n",
    "    doc_term_matrix = vectorizer.fit_transform(raw_documents)\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    \n",
    "    # Sum the counts of each term over all the docs\n",
    "    #dist = np.sum(doc_term_matrix.toarray(), axis=0)\n",
    "    #for tag, count in zip(vocab, dist):\n",
    "    #    print count, tag\n",
    "    \n",
    "    return vocab, doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_preprocessing( raw_review, whitelist=None):\n",
    "    \"\"\"\n",
    "    param: whitelist: is the conjugate of stopwords, the whitelist is a positive filter, \n",
    "    only the words included in the whitelist will be included.\"\"\"\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw HTML review), and \n",
    "    # the output is a single string (a preprocessed review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review, \"html.parser\").get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. Remove stop words\n",
    "    #   In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    #stops = set(stopwords.words(\"english\"))                   \n",
    "    #words = [w for w in words if not w in stops]   \n",
    "    # \n",
    "    # 5. Include white list words\n",
    "    if whitelist:\n",
    "        whitelist = set(whitelist)                   \n",
    "        words = [w for w in words if w in whitelist]   \n",
    "    #\n",
    "    # 7. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( words ))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_reviews = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 25000 of 165975\n",
      "Review 50000 of 165975\n",
      "Review 75000 of 165975\n",
      "Review 100000 of 165975\n",
      "Review 125000 of 165975\n",
      "Review 150000 of 165975\n"
     ]
    }
   ],
   "source": [
    "prep_reviews = []\n",
    "num_reviews = len(raw_reviews)\n",
    "for i, review in enumerate(raw_reviews):\n",
    "    if( (i+1)%25000 == 0 ):\n",
    "        print \"Review %d of %d\" % ( i+1, num_reviews )                                                                    \n",
    "    prep_reviews.append(text_preprocessing(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab, X = extract_features(prep_reviews, 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 ['aa', 'aaa', 'aaaa', 'aaaaa', 'aaaaacd', 'aaaaahhhh', 'aaaaall', 'aaaaarrrrggghhhhthese', 'aaaahhhs', 'aaaallll', 'aaaand', 'aaaggees', 'aaah', 'aaahh', 'aaahs', 'aaai', 'aaargh', 'aaarghdo', 'aaas', 'aaasdfdfdfdfdfdfdfdfdfdfdfdfdfdfddd', 'aaasonly', 'aacells', 'aaddition', 'aae', 'aahem', 'aahhing', 'aahs', 'aaliyah', 'aalma', 'aamuch', 'aamzon', 'aand', 'aap', 'aapguidelinequoting', 'aaps', 'aardvark', 'aarghthe', 'aas', 'aasdont', 'aaseasy', 'aasi', 'aavailable', 'aawwws', 'ab', 'ababy', 'ababybjoumlrn', 'ababycom', 'ababys', 'aback', 'abandon', 'abandoned', 'abandoning', 'abandonment', 'abar', 'abasic', 'abassinet', 'abated', 'abattoiroh', 'abbington', 'abbout', 'abbrasivei', 'abbreviated', 'abby', 'abc', 'abcd', 'abck', 'abcs', 'abd', 'abdc', 'abdomen', 'abdoment', 'abdomin', 'abdominal', 'abdominalmuscles', 'abduct', 'abducted', 'abduction', 'abe', 'abed', 'abel', 'abena', 'aberrations', 'abesolute', 'abf', 'abg', 'abhor', 'abhore', 'abhorent', 'abide', 'abig', 'abilities', 'abilitiesbehavior', 'abilitiesi', 'abilitiesthe', 'ability', 'abilitya', 'abilityi', 'abilitys', 'abilitythis', 'abilty'] 100000\n"
     ]
    }
   ],
   "source": [
    "# remove the u'' prefix in unicode strings\n",
    "feature_names = json.dumps(vocab).translate(None, '\"[] ').split(',')\n",
    "print len(vocab), feature_names[0:100], len(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def as_dictionary(vocab, sparse_x_i):\n",
    "    \"\"\"\n",
    "    Converts the sparse feature array of a sample to a dictionaries.\n",
    "    This function is just to help print the features in a human-interpretable format.\n",
    "    \"\"\"\n",
    "    x_i = sparse_x_i.toarray()[0]\n",
    "    feature_dict = {}\n",
    "    for j, w in enumerate(vocab):\n",
    "        if x_i[j] > 0:\n",
    "            feature_dict[w] = x_i[j]\n",
    "    \n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'playpen': 1, 'getting': 1, 'started': 1, 'money': 1, 'unraveling': 1, 'cheaper': 1, 'wash': 1, 'sheets': 1, 'recommend': 1, 'worth': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I would recommend getting cheaper sheets made for a playpen. These started unraveling after one wash! Not worth the money.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#products['word_count'] = [row.toarray() for row in X] # <-- this is a fast way to crash the kernel\n",
    "print as_dictionary(feature_names, X[269])\n",
    "products.iloc[269]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Sentiment Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a logistic classifier, we need labels. In this case, because it is going to be a sentiment classifier, we want a sentiment label. We will assume that the star rating is equivalent to a sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the **label** (sentiment column), we assign class +1 to star rating of 4 or 5 and -1 to star rating of 1 or 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will split a position array to access the Numpy ndarray. Having the split on the position array is enough and I dont need to actually split X and y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case I leave most columns of the original dataset in the Pandas dataframe, and store the extracted features (wordcounts) in a Numpy ndarray for the ML algorithm.\n",
    "\n",
    "This presents a problem when we split the numpy array into two randomly chosen sets, because I would loose the hability to know which row in the dataframe correspond to each row in the numpy array.\n",
    "\n",
    "In order to be able to refer back to the original row in the df from any row in the numpy arrays, I will split an array of positional indices.\n",
    "\n",
    "Alternatively I could convert all the text columns to char arrays and concatenate these columns to the Numpy ndarray, but all that extra data copying can be avoided by just working with indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132780, 100000) (33195, 100000) [ 1  1 -1  1  1 -1  1  1  1 -1] 21080\n",
      "132780 33195 <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "y = products['sentiment'].values\n",
    "\n",
    "pos = np.array(range(len(products)))\n",
    "pos_train, pos_test = train_test_split(pos, test_size=0.2, random_state=1)\n",
    "\n",
    "print X[pos_train].shape, X[pos_test].shape, y[pos_train][:10], sum(y[pos_train]<0)\n",
    "print len(pos_train), len(pos_test), type(pos_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the sentiment classifier with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_model = linear_model.LogisticRegression()\n",
    "# fit() requires X features in scipy compressed sparse format, fit_transform() takes numpy ndarrays\n",
    "sentiment_model.fit(X[pos_train], y[pos_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100000)\n",
      "[  2.49875619e-01   1.30748533e+00   6.24072804e-04  -1.20713079e-01\n",
      "   8.95634613e-02   6.24686891e-05   3.86380070e-07   3.16755064e-01\n",
      "  -2.49622333e-01   9.12890213e-08]\n",
      "Number of positive weights: 62881 \n",
      "Number of negative weights: 25276 \n",
      "\n",
      "[ 1.18419346]\n",
      "Number of positive intercepts: 1 \n",
      "Number of negative intercepts: 0 \n"
     ]
    }
   ],
   "source": [
    "coef = sentiment_model.coef_[0]\n",
    "inter = sentiment_model.intercept_\n",
    "print sentiment_model.coef_.shape\n",
    "\n",
    "print coef[0:10]\n",
    "num_positive_weights = sum(coef>0)\n",
    "num_negative_weights = sum(coef<0)\n",
    "print \"Number of positive weights: %s \" % num_positive_weights\n",
    "print \"Number of negative weights: %s \" % num_negative_weights\n",
    "print\n",
    "\n",
    "print inter\n",
    "num_positive_inter = sum(inter>0)\n",
    "num_negative_inter = sum(inter<0)\n",
    "print \"Number of positive intercepts: %s \" % num_positive_inter\n",
    "print \"Number of negative intercepts: %s \" % num_negative_inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions with logistic regression\n",
    "\n",
    "Now that a model is trained, we can make predictions on the **test data**. In this section, we will explore this in the context of 3 examples in the test dataset.  We refer to this set of 3 examples as the **sample_test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 94825 100257  30688]\n",
      "{'efficientvery': 1, 'think': 1, 'love': 1, 'dont': 1, 'just': 1, 'urban': 1, 'backwardshigh': 1, 'seat': 3, 'selfproclaimed': 1, 'junkie': 1, 'strollerthe': 1, 'purchased': 1, 'seen': 1, 'quality': 1, 'huge': 1, 'ease': 1, 'pregnant': 1, 'long': 1, 'better': 1, 'read': 1, 'yetits': 1, 'wobbly': 1, 'qualitythe': 1, 'wiped': 1, 'store': 1, 'nice': 1, 'model': 1, 'snaps': 1, 'consonce': 1, 'gear': 1, 'emergency': 1, 'easily': 1, 'positioners': 1, 'loose': 1, 'spot': 1, 'sturdyeasily': 1, 'wave': 2, 'strap': 1, 'connected': 1, 'prossun': 1, 'uselightweighthandle': 1, 'coordinating': 1, 'baby': 2, 'bit': 1, 'stiff': 1, 'stylishfeels': 1, 'easiest': 1, 'cleanthe': 1, 'boy': 1, 'tricky': 1, 'daycare': 1, 'aroundeasy': 1, 'car': 2, 'inexpensive': 1, 'bargain': 1, 'stroller': 2, 'try': 1, 'materials': 1, 'canopy': 1, 'obsess': 1, 'review': 1, 'say': 2, 'settings': 1}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>104808</th>\n",
       "      <td>The First Years Via I450 Infant Carseat, Urban...</td>\n",
       "      <td>I must say I am in love with this car seat. I ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110790</th>\n",
       "      <td>Medela 150 Ml Storage Bottle Case of 10 BPA FREE</td>\n",
       "      <td>Received this product and was a bit worried to...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33772</th>\n",
       "      <td>Lamaze Play &amp;amp; Grow My Friend Emily Take Al...</td>\n",
       "      <td>a great toy for new born babies, has lot of st...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "104808  The First Years Via I450 Infant Carseat, Urban...   \n",
       "110790   Medela 150 Ml Storage Bottle Case of 10 BPA FREE   \n",
       "33772   Lamaze Play &amp; Grow My Friend Emily Take Al...   \n",
       "\n",
       "                                                   review  rating  sentiment  \n",
       "104808  I must say I am in love with this car seat. I ...       5          1  \n",
       "110790  Received this product and was a bit worried to...       1         -1  \n",
       "33772   a great toy for new born babies, has lot of st...       4          1  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#idx_test_sample = idx_test[14:17]\n",
    "pos_test_sample = pos_test[15:18]\n",
    "#print idx_test_sample\n",
    "print pos_test_sample\n",
    "print as_dictionary(feature_names, X[pos_test_sample[0]])\n",
    "products.iloc[pos_test_sample]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig deeper into the first row of the **sample_test_data**. Here's the full review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I must say I am in love with this car seat. I am pregnant with baby boy #3 and I am a self-proclaimed baby gear junkie. I read every review, try every model in the store and obsess over every detail. We also purchased the coordinating Wave Urban Stroller.The pros:sun canopy is efficientvery stylishfeels very sturdyeasily wiped cleanthe car seat snaps on easily to the stroller (as long as you don't have it backwards)high quality materials all aroundeasy to uselightweighthandle has spot for emergency & daycare inforoomy seat with nice positioners for newbornschanging strap settings is the easiest i have seen yetIt's so inexpensive- this is a huge bargain for the qualitythe cons:once connected to the wave stroller it was a bit tricky to get off, but I think it's just stiff and will ease up, which I still say is better than being wobbly and loose.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.iloc[pos_test_sample[0]]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That review seems pretty positive.\n",
    "\n",
    "Now, let's see what the next row of the **sample_test_data** looks like. As we could guess from the sentiment (-1), the review is quite negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Received this product and was a bit worried to use it because I came in a zip lock type bag that was not even sealed so half of the bottles fell out into the box it was shipped in.  Was hesitant to use it so I am returning it.\n"
     ]
    }
   ],
   "source": [
    "print products.iloc[pos_test_sample[1]]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now make a **class** prediction for the **sample_test_data**. \n",
    "The `sentiment_model` should predict **+1** if the sentiment is positive and **-1** if the sentiment is negative. The **score** (sometimes called **margin**) for the logistic regression model  is defined as:\n",
    "\n",
    "$$\n",
    "\\mbox{score}_i = \\mathbf{w}^T h(\\mathbf{x}_i)\n",
    "$$ \n",
    "\n",
    "where $h(\\mathbf{x}_i)$ represents the features for example $i$.  We will write some code to obtain the **scores**. For each row, the **score** (or margin) is a number in the range **[-inf, inf]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting sentiment\n",
    "\n",
    "These scores can be used to make class predictions as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{w}^T h(\\mathbf{x}_i) > 0 \\\\\n",
    "      -1 & \\mathbf{w}^T h(\\mathbf{x}_i) \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "Where whe have included -b as w[0]\n",
    "\n",
    "Using scores, write code to calculate $\\hat{y}$, the class predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculation of predictions with formula (w.x - b > 0)\n",
      "Types: <type 'numpy.ndarray'> <class 'scipy.sparse.csr.csr_matrix'>\n",
      "Dimension alignment for dot product: (3, 100000) (100000, 1)\n"
     ]
    }
   ],
   "source": [
    "print \"Calculation of predictions with formula (w.x - b > 0)\" \n",
    "h_x = X[pos_test_sample]\n",
    "w = sentiment_model.coef_\n",
    "b = sentiment_model.intercept_\n",
    "print 'Types:', type(w),  type(h_x)\n",
    "print 'Dimension alignment for dot product:', h_x.shape, w.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores:\n",
      "5.06516649448 \n",
      "-3.05514722041 \n",
      "0.803633184553\n"
     ]
    }
   ],
   "source": [
    "scores =  np.dot(h_x.toarray(), w.T) - b\n",
    "print \"Scores:\" \n",
    "print \" \\n\".join([str(s[0]) for s in scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions (with formula w.x - b > 0):\n",
      "[1, -1, 1]\n"
     ]
    }
   ],
   "source": [
    "print \"Class predictions (with formula w.x - b > 0):\" \n",
    "print map(lambda s: 1 if s>0 else -1, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions (scikit-learn):\n",
      "[ 1 -1  1]\n"
     ]
    }
   ],
   "source": [
    "print \"Class predictions (scikit-learn):\" \n",
    "y_hats = sentiment_model.predict(X[pos_test_sample])\n",
    "print y_hats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability predictions\n",
    "\n",
    "We can also calculate the probability predictions from the scores using:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))}.\n",
    "$$\n",
    "\n",
    "Using the variable **scores** calculated previously, write code to calculate the probability that a sentiment is positive using the above formula. For each row, the probabilities should be a number in the range **[0, 1]**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class probability predictions (scikit-learn):\n",
      "          -1              1\n",
      "[[  5.90734235e-04   9.99409266e-01]\n",
      " [  6.65245851e-01   3.34754149e-01]\n",
      " [  4.02323390e-02   9.59767661e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Returns the probability of the sample for each class in the mode\n",
    "proba_sample = sentiment_model.predict_proba(X[pos_test_sample])\n",
    "\n",
    "print \"Class probability predictions (scikit-learn):\" \n",
    "print \"          -1              1\" \n",
    "print proba_sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the three data points in **sample_test_data**, the second sample has the **lowest probability** of being classified as a positive review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the most positive (and negative) review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the 20 with the **highest probability** of being classified as a **positive review**.\n",
    "1.  Make probability predictions on all the test set  **X_test** \n",
    "2.  Sort the results descendently by the probability of class '1' and pick the top 20 \n",
    "    (or sort ascendently and pick the last 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class probability predictions (scikit-learn):\n",
      "     -1              1\n",
      "[[  2.01660624e-06   9.99997983e-01]\n",
      " [  7.94329813e-03   9.92056702e-01]\n",
      " [  5.47455478e-01   4.52544522e-01]\n",
      " [  9.88904233e-01   1.10957670e-02]\n",
      " [  1.27436044e-01   8.72563956e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Returns the probability of the sample for each class in the mode\n",
    "proba_test = sentiment_model.predict_proba(X[pos_test])\n",
    "\n",
    "print \"Class probability predictions (scikit-learn):\" \n",
    "print \"     -1              1\" \n",
    "print proba_test[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        -1              1\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>114796</th>\n",
       "      <td>Fisher-Price Cradle 'N Swing,  My Little Snuga...</td>\n",
       "      <td>My husband and I cannot state enough how much ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95572</th>\n",
       "      <td>Peg-Perego 2010 Gt3 for Two Performance Stroll...</td>\n",
       "      <td>I should probably start out by saying that I a...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54234</th>\n",
       "      <td>UPPAbaby PiggyBack Ride Along Board</td>\n",
       "      <td>We love our Vista with PiggyBack.  There are a...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42430</th>\n",
       "      <td>bumGenius One-Size Cloth Diaper Twilight</td>\n",
       "      <td>new to cloth diapering- trying to figure out i...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133651</th>\n",
       "      <td>Britax 2012 B-Agile Stroller, Red</td>\n",
       "      <td>[I got this stroller for my daughter prior to ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152013</th>\n",
       "      <td>UPPAbaby Vista Stroller, Denny</td>\n",
       "      <td>I researched strollers for months and months b...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111746</th>\n",
       "      <td>Baby Jogger 2011 City Mini Double Stroller, Bl...</td>\n",
       "      <td>Before purchasing this stroller, I read severa...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168697</th>\n",
       "      <td>Graco FastAction Fold Jogger Click Connect Str...</td>\n",
       "      <td>Graco's FastAction Jogging Stroller definitely...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79357</th>\n",
       "      <td>WubbaNub Tabby Kitten</td>\n",
       "      <td>I first bought these when I again had to repla...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168081</th>\n",
       "      <td>Buttons Cloth Diaper Cover - One Size - 8 Colo...</td>\n",
       "      <td>We are big Best Bottoms fans here, but I wante...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180953</th>\n",
       "      <td>Baby Jogger City Lite Stroller, Black</td>\n",
       "      <td>AMAZING stroller!  It took me about 2 minutes ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119182</th>\n",
       "      <td>Roan Rocco Classic Pram Stroller 2-in-1 with B...</td>\n",
       "      <td>Great Pram Rocco!!!!!!I bought this pram from ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60298</th>\n",
       "      <td>Ju-Ju-Be Be Right Back Backpack Diaper Bag, Bl...</td>\n",
       "      <td>This review is going to compare 3 Ju-Ju-Be bag...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66059</th>\n",
       "      <td>Evenflo 6 Pack Classic Glass Bottle, 4-Ounce</td>\n",
       "      <td>It's always fun to write a review on those pro...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129212</th>\n",
       "      <td>Baby Jogger 2011 City Select Stroller in Ameth...</td>\n",
       "      <td>I have the Baby Jogger City Select with Second...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108803</th>\n",
       "      <td>Chicco Keyfit 22 Pound Infant Car Seat And Bas...</td>\n",
       "      <td>I bought this right before the KeyFit 30 came ...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167047</th>\n",
       "      <td>Inglesina 2013 Trip Stroller, Lampone Purple</td>\n",
       "      <td>I did many hours of research reading reviews a...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142686</th>\n",
       "      <td>Thirsties Duo Diaper Snap, Mud, Size One (6-18...</td>\n",
       "      <td>I will try to keep this short.  Have tried man...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47813</th>\n",
       "      <td>Baby K'tan Baby Carrier, Black, X-Large</td>\n",
       "      <td>Check and recheck the K'Tan for size issues be...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93170</th>\n",
       "      <td>Contours Options Tandem II Stroller, Tangerine</td>\n",
       "      <td>We have 7 month old twins, and have used it si...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "114796  Fisher-Price Cradle 'N Swing,  My Little Snuga...   \n",
       "95572   Peg-Perego 2010 Gt3 for Two Performance Stroll...   \n",
       "54234                 UPPAbaby PiggyBack Ride Along Board   \n",
       "42430            bumGenius One-Size Cloth Diaper Twilight   \n",
       "133651                  Britax 2012 B-Agile Stroller, Red   \n",
       "152013                     UPPAbaby Vista Stroller, Denny   \n",
       "111746  Baby Jogger 2011 City Mini Double Stroller, Bl...   \n",
       "168697  Graco FastAction Fold Jogger Click Connect Str...   \n",
       "79357                               WubbaNub Tabby Kitten   \n",
       "168081  Buttons Cloth Diaper Cover - One Size - 8 Colo...   \n",
       "180953              Baby Jogger City Lite Stroller, Black   \n",
       "119182  Roan Rocco Classic Pram Stroller 2-in-1 with B...   \n",
       "60298   Ju-Ju-Be Be Right Back Backpack Diaper Bag, Bl...   \n",
       "66059        Evenflo 6 Pack Classic Glass Bottle, 4-Ounce   \n",
       "129212  Baby Jogger 2011 City Select Stroller in Ameth...   \n",
       "108803  Chicco Keyfit 22 Pound Infant Car Seat And Bas...   \n",
       "167047       Inglesina 2013 Trip Stroller, Lampone Purple   \n",
       "142686  Thirsties Duo Diaper Snap, Mud, Size One (6-18...   \n",
       "47813             Baby K'tan Baby Carrier, Black, X-Large   \n",
       "93170      Contours Options Tandem II Stroller, Tangerine   \n",
       "\n",
       "                                                   review  rating  sentiment  \n",
       "114796  My husband and I cannot state enough how much ...       5          1  \n",
       "95572   I should probably start out by saying that I a...       4          1  \n",
       "54234   We love our Vista with PiggyBack.  There are a...       5          1  \n",
       "42430   new to cloth diapering- trying to figure out i...       5          1  \n",
       "133651  [I got this stroller for my daughter prior to ...       4          1  \n",
       "152013  I researched strollers for months and months b...       5          1  \n",
       "111746  Before purchasing this stroller, I read severa...       5          1  \n",
       "168697  Graco's FastAction Jogging Stroller definitely...       5          1  \n",
       "79357   I first bought these when I again had to repla...       5          1  \n",
       "168081  We are big Best Bottoms fans here, but I wante...       4          1  \n",
       "180953  AMAZING stroller!  It took me about 2 minutes ...       5          1  \n",
       "119182  Great Pram Rocco!!!!!!I bought this pram from ...       5          1  \n",
       "60298   This review is going to compare 3 Ju-Ju-Be bag...       5          1  \n",
       "66059   It's always fun to write a review on those pro...       5          1  \n",
       "129212  I have the Baby Jogger City Select with Second...       5          1  \n",
       "108803  I bought this right before the KeyFit 30 came ...       5          1  \n",
       "167047  I did many hours of research reading reviews a...       5          1  \n",
       "142686  I will try to keep this short.  Have tried man...       5          1  \n",
       "47813   Check and recheck the K'Tan for size issues be...       5          1  \n",
       "93170   We have 7 month old twins, and have used it si...       5          1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick the top 20: sorting ascendently and select the last 20\n",
    "# call np.argsort() passing the column which is the sort key, it returns an array of row indices \n",
    "# that sort the matrix by the key column when you use it as index to access the matrix.\n",
    "proba_test_sort_ascen = np.argsort(proba_test[:, 1])\n",
    "print \"        -1              1\" \n",
    "print proba_test[proba_test_sort_ascen[-5:]]\n",
    "orig_pos = pos_test[proba_test_sort_ascen[-20:]]\n",
    "products.iloc[orig_pos]  # iloc is by position, the i stands by 'integer', not 'index'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Which of the following products are represented in the 20 most positive reviews? [multiple choice]\n",
    "\n",
    "\n",
    "Now, let us repeat this excercise to find the \"most negative reviews.\" Use the prediction probabilities to find the  20 reviews in the **test_data** with the **lowest probability** of being classified as a **positive review**. Repeat the same steps above but make sure you **sort in the opposite order**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      -1              1\n",
      "[[  1.00000000e+00   7.40554082e-11]\n",
      " [  1.00000000e+00   2.72453108e-11]\n",
      " [  1.00000000e+00   4.44859718e-12]\n",
      " [  1.00000000e+00   2.09069217e-13]\n",
      " [  1.00000000e+00   1.69573589e-20]]\n"
     ]
    }
   ],
   "source": [
    "# sorting in reverse order [::-1]\n",
    "# sorting by the same column, just get the rows with \n",
    "# the lowest probability of being classified as a positive review\n",
    "proba_test_sort_desc = np.argsort(proba_test[:, 1])[::-1]\n",
    "print \"      -1              1\" \n",
    "print proba_test[proba_test_sort_desc[-5:]] \n",
    "orig_pos = pos_test[proba_test_sort_desc[-20:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>review</th>\n",
       "      <th>rating</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7075</th>\n",
       "      <td>Peace of Mind Two 900 Mhz Baby Receivers, Monitor</td>\n",
       "      <td>If we only knew when we registered how terribl...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138762</th>\n",
       "      <td>Fuzzi Bunz Diaper Sprayer - White</td>\n",
       "      <td>Purchased from &amp;#34;perrymerchant&amp;#34;This spr...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18599</th>\n",
       "      <td>Eddie Bauer Classic Wood Swing</td>\n",
       "      <td>What an awful product! The motor failed after ...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36569</th>\n",
       "      <td>Summer Infant Secure Sounds Digital Multi Room...</td>\n",
       "      <td>We had a multi-room baby monitor before, but i...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110305</th>\n",
       "      <td>Eco Vessel Kids Stainless Steel Water Bottle w...</td>\n",
       "      <td>I was looking forward to receiving three quali...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179608</th>\n",
       "      <td>Diy Monthly Chalkboard Calendar Vinyl Wall Dec...</td>\n",
       "      <td>Fantastic concept.  Awful execution.  Don't wa...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59546</th>\n",
       "      <td>Ellaroo Mei Tai Baby Carrier - Hershey</td>\n",
       "      <td>This is basically an overpriced piece of fabri...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37099</th>\n",
       "      <td>The First Years Clean Air Diaper Disposal System</td>\n",
       "      <td>Unbelievable... that's what I say every time I...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176038</th>\n",
       "      <td>Megaseat Play and Snack Tray, White</td>\n",
       "      <td>This comes in two pieces that sort of snap tog...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134225</th>\n",
       "      <td>Tek-tok 2.5&amp;quot; Color Video Baby Monitor and...</td>\n",
       "      <td>Fellow Parents, I bought the Tek Tok baby moni...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13383</th>\n",
       "      <td>North States Superyard Play Yard, Grey, 6 Panel</td>\n",
       "      <td>We were so disappointed with this enclosure sy...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59312</th>\n",
       "      <td>Evenflo Majestic High Chair</td>\n",
       "      <td>I am not one to write a review, but the Evenfl...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41532</th>\n",
       "      <td>Philips Avent Digital Two-Parent Baby Monitor ...</td>\n",
       "      <td>Up-front summary: This attractive product has ...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134999</th>\n",
       "      <td>Infant Optics DXR-5 2.4 GHz Digital Video Baby...</td>\n",
       "      <td>Let me begin with the fact that the monitor wo...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121464</th>\n",
       "      <td>Ameda Purely Yours Breast Pump</td>\n",
       "      <td>This pump worked really well for less than a w...</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12643</th>\n",
       "      <td>900 MHz Home Connection Monitor</td>\n",
       "      <td>When I received the Home Connection Monitor on...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66359</th>\n",
       "      <td>Levana BABYVIEW20 Interference Free Digital Wi...</td>\n",
       "      <td>where do i even begin? this baby monitor is no...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10370</th>\n",
       "      <td>Wimmer-Ferguson Infant Stim-Mobile</td>\n",
       "      <td>This product should be in the hall of fame sol...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68194</th>\n",
       "      <td>Evenflo Crosstown Soft Portable Travel Gate</td>\n",
       "      <td>I'm sure that this product passed all the form...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87026</th>\n",
       "      <td>Baby Einstein Around The World Discovery Center</td>\n",
       "      <td>First off, I did manage to find this product f...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     name  \\\n",
       "7075    Peace of Mind Two 900 Mhz Baby Receivers, Monitor   \n",
       "138762                  Fuzzi Bunz Diaper Sprayer - White   \n",
       "18599                      Eddie Bauer Classic Wood Swing   \n",
       "36569   Summer Infant Secure Sounds Digital Multi Room...   \n",
       "110305  Eco Vessel Kids Stainless Steel Water Bottle w...   \n",
       "179608  Diy Monthly Chalkboard Calendar Vinyl Wall Dec...   \n",
       "59546              Ellaroo Mei Tai Baby Carrier - Hershey   \n",
       "37099    The First Years Clean Air Diaper Disposal System   \n",
       "176038                Megaseat Play and Snack Tray, White   \n",
       "134225  Tek-tok 2.5&quot; Color Video Baby Monitor and...   \n",
       "13383     North States Superyard Play Yard, Grey, 6 Panel   \n",
       "59312                         Evenflo Majestic High Chair   \n",
       "41532   Philips Avent Digital Two-Parent Baby Monitor ...   \n",
       "134999  Infant Optics DXR-5 2.4 GHz Digital Video Baby...   \n",
       "121464                     Ameda Purely Yours Breast Pump   \n",
       "12643                     900 MHz Home Connection Monitor   \n",
       "66359   Levana BABYVIEW20 Interference Free Digital Wi...   \n",
       "10370                  Wimmer-Ferguson Infant Stim-Mobile   \n",
       "68194         Evenflo Crosstown Soft Portable Travel Gate   \n",
       "87026     Baby Einstein Around The World Discovery Center   \n",
       "\n",
       "                                                   review  rating  sentiment  \n",
       "7075    If we only knew when we registered how terribl...       1         -1  \n",
       "138762  Purchased from &#34;perrymerchant&#34;This spr...       1         -1  \n",
       "18599   What an awful product! The motor failed after ...       1         -1  \n",
       "36569   We had a multi-room baby monitor before, but i...       2         -1  \n",
       "110305  I was looking forward to receiving three quali...       1         -1  \n",
       "179608  Fantastic concept.  Awful execution.  Don't wa...       1         -1  \n",
       "59546   This is basically an overpriced piece of fabri...       1         -1  \n",
       "37099   Unbelievable... that's what I say every time I...       1         -1  \n",
       "176038  This comes in two pieces that sort of snap tog...       1         -1  \n",
       "134225  Fellow Parents, I bought the Tek Tok baby moni...       1         -1  \n",
       "13383   We were so disappointed with this enclosure sy...       1         -1  \n",
       "59312   I am not one to write a review, but the Evenfl...       1         -1  \n",
       "41532   Up-front summary: This attractive product has ...       4          1  \n",
       "134999  Let me begin with the fact that the monitor wo...       1         -1  \n",
       "121464  This pump worked really well for less than a w...       2         -1  \n",
       "12643   When I received the Home Connection Monitor on...       1         -1  \n",
       "66359   where do i even begin? this baby monitor is no...       1         -1  \n",
       "10370   This product should be in the hall of fame sol...       1         -1  \n",
       "68194   I'm sure that this product passed all the form...       1         -1  \n",
       "87026   First off, I did manage to find this product f...       1         -1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.iloc[orig_pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that many of the reviews for which we  predict a negative sentiment have a high start rating, this may be because our classifier is detecting negative sentiment in the words although the person writing the review is nice and doesn't want to give a bad rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute accuracy of the classifier\n",
    "\n",
    "We will now evaluate the accuracy of the trained classifer. Recall that the accuracy is given by\n",
    "\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified examples}}{\\mbox{# total examples}}\n",
    "$$\n",
    "\n",
    "This can be computed as follows:\n",
    "\n",
    "* **Step 1:** Use the trained model to compute class predictions\n",
    "* **Step 2:** Count the number of data points when the predicted class labels match the ground truth labels (called `true_labels` below).\n",
    "* **Step 3:** Divide the total number of correct predictions by the total number of data points in the dataset.\n",
    "\n",
    "Complete the function below to compute the classification accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_classification_accuracy(model, data, true_labels):\n",
    "    # First get the predictions\n",
    "    y_hat = model.predict(data)\n",
    "    \n",
    "    # Compute the number of correctly classified examples (true negatives plus true positives)\n",
    "    tn_tp = sum(y_hat == true_labels)\n",
    "\n",
    "    # Then compute accuracy by dividing num_correct by total number of examples\n",
    "    accuracy = tn_tp / float(data.shape[0])\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the classification accuracy of the **sentiment_model** on the **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data 0.96\n",
      "Accuracy on test data 0.92\n"
     ]
    }
   ],
   "source": [
    "ac_train = get_classification_accuracy(sentiment_model, X[pos_train], y[pos_train])\n",
    "print \"Accuracy on training data %.2f\" % ac_train\n",
    "ac_test = get_classification_accuracy(sentiment_model, X[pos_test], y[pos_test])\n",
    "print \"Accuracy on test data %.2f\" % ac_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A higher accuracy value on the **training_data** not always imply that the classifier is better, it could be overfitting and loosing generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn another classifier with fewer words\n",
    "\n",
    "There were a lot of words in the model we trained above. We will now train a simpler logistic regression model using only a subet of words that occur in the reviews. For this assignment, we selected a 20 words to work with. These are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(significant_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each review, we will use the **word_count** column and trim out all words that are **not** in the **significant_words** list above. Note that we are performing this on both the training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 25000 of 165975\n",
      "Review 50000 of 165975\n",
      "Review 75000 of 165975\n",
      "Review 100000 of 165975\n",
      "Review 125000 of 165975\n",
      "Review 150000 of 165975\n"
     ]
    }
   ],
   "source": [
    "# preprocess\n",
    "white_reviews = []\n",
    "num_reviews = len(raw_reviews)\n",
    "for i, review in enumerate(raw_reviews):\n",
    "    if( (i+1)%25000 == 0 ):\n",
    "        print \"Review %d of %d\" % ( i+1, num_reviews )                                                                    \n",
    "    white_reviews.append(text_preprocessing(review, significant_words))\n",
    "\n",
    "# feature extraction\n",
    "white_vocab, white_X = extract_features(white_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['able', 'broke', 'car', 'disappointed', 'easy', 'even', 'great', 'less', 'little', 'love', 'loves', 'money', 'old', 'perfect', 'product', 'return', 'waste', 'well', 'work', 'would']\n",
      "4 words have been removed as stop words: well, even, less, would\n",
      "{'money': 1, 'would': 1}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect the wordcounts\n",
    "white_feature_names = json.dumps(white_vocab).translate(None, '\"[] ').split(',')\n",
    "print len(white_feature_names), white_feature_names\n",
    "print \"4 words have been removed as stop words: well, even, less, would\"\n",
    "print as_dictionary(white_feature_names, white_X[269])\n",
    "\n",
    "# split the new features\n",
    "# no need to split white_X because we reuse the pos index partition\n",
    "\n",
    "# train the new model\n",
    "white_sentiment_model = linear_model.LogisticRegression()\n",
    "white_sentiment_model.fit(white_X[pos_train], y[pos_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the first example of the dataset looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'We had bought the Avent bottles & they gave our little girl colic.  So we switched to these & what a difference.  They have a super cute design too.  Love them & would highly recommend them.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print pos_train[0]\n",
    "products.iloc[pos_train[0]]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **word_count** column had been working with before looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cute': 1, 'little': 1, 'difference': 1, 'bottles': 1, 'love': 1, 'switched': 1, 'highly': 1, 'avent': 1, 'design': 1, 'recommend': 1, 'gave': 1, 'girl': 1, 'bought': 1, 'super': 1, 'colic': 1}\n"
     ]
    }
   ],
   "source": [
    "print as_dictionary(feature_names, X[pos_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are only working with a subet of these words, the column **word_count_subset** is a subset of the above dictionary. In this example, only 2 `significant words` are present in this review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'little': 1, 'love': 1, 'would': 1}\n"
     ]
    }
   ],
   "source": [
    "print as_dictionary(white_feature_names, white_X[pos_train[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the classification accuracy using the `get_classification_accuracy` function implemented earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data 0.87\n",
      "Accuracy on test data 0.87\n"
     ]
    }
   ],
   "source": [
    "ac_train = get_classification_accuracy(white_sentiment_model, white_X[pos_train], y[pos_train])\n",
    "print \"Accuracy on training data %.2f\" % ac_train\n",
    "ac_test = get_classification_accuracy(white_sentiment_model, white_X[pos_test], y[pos_test])\n",
    "print \"Accuracy on test data %.2f\" % ac_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will inspect the weights (coefficients) of the **simple_model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "[ 0.22286894 -1.642837    0.08412074 -2.40920758  1.18133062 -0.52189432\n",
      "  0.93987636 -0.18517016  0.51997707  1.34524368]\n",
      "Number of positive weights: 10 \n",
      "Number of negative weights: 10 \n",
      "\n",
      "[ 1.29005289]\n",
      "Number of positive intercepts: 1 \n",
      "Number of negative intercepts: 0 \n"
     ]
    }
   ],
   "source": [
    "white_coef = white_sentiment_model.coef_[0]\n",
    "white_inter = white_sentiment_model.intercept_\n",
    "print white_sentiment_model.coef_.shape\n",
    "\n",
    "print white_coef[0:10]\n",
    "num_positive_white = sum(white_coef>0)\n",
    "num_negative_white = sum(white_coef<0)\n",
    "print \"Number of positive weights: %s \" % num_positive_white\n",
    "print \"Number of negative weights: %s \" % num_negative_white\n",
    "print\n",
    "\n",
    "print white_inter\n",
    "num_positive_white_inter = sum(white_inter>0)\n",
    "num_negative_white_inter = sum(white_inter<0)\n",
    "print \"Number of positive intercepts: %s \" % num_positive_white_inter\n",
    "print \"Number of negative intercepts: %s \" % num_negative_white_inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sort the coefficients (in descending order) by the **value** to obtain the coefficients with the most positive effect on the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.40920758 -2.06217068 -1.97669413 -1.642837   -0.92120186 -0.63278865\n",
      " -0.52189432 -0.33844415 -0.31477555 -0.18517016  0.08412074  0.08970073\n",
      "  0.22286894  0.51697152  0.51997707  0.93987636  1.18133062  1.34524368\n",
      "  1.46918496  1.72105392]\n"
     ]
    }
   ],
   "source": [
    "xs = np.argsort(white_coef)\n",
    "print white_coef[xs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the positive words in the **simple_model** (let us call them `positive_significant_words`) also positive words in the **sentiment_model**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love\t\t0.2228689353\t\t1.6127443365\t\tlove\n",
      "easy\t\t0.0841207424\t\t1.3131702092\t\teasy\n",
      "little\t\t1.1813306155\t\t0.6874813956\t\tlittle\n",
      "loves\t\t0.9398763577\t\t1.7471269759\t\tloves\n",
      "able\t\t0.5199770713\t\t0.2671479952\t\table\n",
      "car\t\t1.3452436801\t\t0.1934654377\t\tcar\n",
      "broke\t\t1.7210539168\t\t-1.4537817255\t\tbroke\n",
      "even\t\t0.0897007266\t\t0.0000000000\t\t*stop word*\n",
      "waste\t\t1.4691849582\t\t-2.0790057659\t\twaste\n",
      "money\t\t0.5169715209\t\t-0.7805717827\t\tmoney\n"
     ]
    }
   ],
   "source": [
    "for iwhite, wwhite in enumerate(significant_words):\n",
    "    if white_coef[iwhite] > 0:\n",
    "        i = feature_names.index(wwhite) if wwhite in feature_names else -1\n",
    "        print \"%s\\t\\t%1.10f\\t\\t%1.10f\\t\\t%s\" % (wwhite, white_coef[iwhite], \n",
    "                                                coef[i] if i>0 else 0, feature_names[i] if i>0 else '*stop word*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now compare the accuracy of the **sentiment_model** and the **simple_model** using the `get_classification_accuracy` method you implemented above.\n",
    "\n",
    "First, compute the classification accuracy of the **sentiment_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data 0.96\n"
     ]
    }
   ],
   "source": [
    "ac_train = get_classification_accuracy(sentiment_model, X[pos_train], y[pos_train])\n",
    "print \"Accuracy on training data %.2f\" % ac_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the classification accuracy of the **simple_model** on the **train_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on whitelisted training data 0.87\n"
     ]
    }
   ],
   "source": [
    "ac_train_white = get_classification_accuracy(white_sentiment_model, white_X[pos_train], y[pos_train])\n",
    "print \"Accuracy on whitelisted training data %.2f\" % ac_train_white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TRAINING set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will repeat this excercise on the **test_data**. Start by computing the classification accuracy of the **sentiment_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data 0.92\n"
     ]
    }
   ],
   "source": [
    "ac_test = get_classification_accuracy(sentiment_model, X[pos_test], y[pos_test])\n",
    "print \"Accuracy on test data %.2f\" % ac_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will compute the classification accuracy of the **simple_model** on the **test_data**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on whitelisted test data 0.87\n"
     ]
    }
   ],
   "source": [
    "ac_test_white = get_classification_accuracy(white_sentiment_model, white_X[pos_test], y[pos_test])\n",
    "print \"Accuracy on whitelisted test data %.2f\" % ac_test_white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Which model (**sentiment_model** or **simple_model**) has higher accuracy on the TEST set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Majority class prediction\n",
    "\n",
    "It is quite common to use the **majority class classifier** as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.\n",
    "\n",
    "What is the majority class in the **train_data**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111700\n",
      "21080\n",
      "Mayority class classifier:  predict always positive\n"
     ]
    }
   ],
   "source": [
    "num_positive  = (y[pos_train] == +1).sum()\n",
    "num_negative = (y[pos_train] == -1).sum()\n",
    "print num_positive\n",
    "print num_negative\n",
    "print 'Mayority class classifier:  predict always', 'positive' if num_positive > num_negative else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the accuracy of the majority class classifier on **test_data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of mayrity class classifier on test data 0.84\n"
     ]
    }
   ],
   "source": [
    "def get_classification_accuracy_mcc(true_labels):\n",
    "    # First get the predictions\n",
    "    y_hat = np.ones(len(true_labels))\n",
    "    \n",
    "    # Compute the number of correctly classified examples (true negatives plus true positives)\n",
    "    tn_tp = sum(y_hat == true_labels)\n",
    "\n",
    "    # Then compute accuracy by dividing num_correct by total number of examples\n",
    "    accuracy = tn_tp / float(len(true_labels))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "ac_test_mcc = get_classification_accuracy_mcc( y[pos_test])\n",
    "print \"Accuracy of mayrity class classifier on test data %.2f\" % ac_test_mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
